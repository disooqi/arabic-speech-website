{% extends "layout.html" %}

{% block content %}
<div class="col-md-8 blog-main">
    <br/>
    <br/>
    <nav>
  <div class="nav nav-tabs" id="nav-tab" role="tablist">
    <a class="nav-item nav-link active" id="nav-asr-tab" data-toggle="tab" href="#nav-asr" role="tab" aria-controls="nav-asr" aria-selected="true"><small>Automatic Speech Recognition</small></a>
    <a class="nav-item nav-link" id="nav-adi-tab" data-toggle="tab" href="#nav-adi" role="tab" aria-controls="nav-adi" aria-selected="false"><small>Arabic Dialect Identifcation</small></a>
    <a class="nav-item nav-link" id="nav-tts-tab" data-toggle="tab" href="#nav-tts" role="tab" aria-controls="nav-tts" aria-selected="false"><small>Text To Speech</small></a>
    <a class="nav-item nav-link" id="nav-lm-tab" data-toggle="tab" href="#nav-lm" role="tab" aria-controls="nav-lm" aria-selected="false"><small>Language Model</small></a>
  </div>
</nav>
<div class="tab-content" id="nav-tabContent">
  <div class="tab-pane fade show active" id="nav-asr" role="tabpanel" aria-labelledby="nav-asr-tab">
  <div class="blog-post">
                        <br/>
                        <h4 class="">Automatic Speech Recognition</h4>
                        <br/>
                        <p>Automatic Speech Recognition (ASR) is the process of converting the speech signal into its corresponding text. The quality of ASR systems is measured by how close their recognized sequences of words are to human recognized sequences of words. More formally, the ASR quality, measured by Word Error Rate (WER), is the edit-distance between automatically generated hypotheses and the ground-truth human transcription.</p>
                        <p>Traditionally, ASR systems are split into four components: the acoustic model, the pronunciation model, the language model, and the  search decoder. Since ASR output hypotheses need to adhere to the statistical structure of language, the language model ensure that the output sequence matches what is likely to be said, e.g. words like "school" or "work" have higher probability than "oil" or "yellow" in the sentence: "Ali walks to his ....... ". The pronunciation dictionary is used to decompose words into small units of sound, known as Phonemes. The acoustic model represents the mapping between the audio signal, its temporal, i.e. time-related, and spectral, i.e. frequency-related, characteristics and different phonemes in language. Each model assigns probabilities to different choices it makes then the decoder searches over all these alternatives weighing their probabilities to come up with the best output hypothesis. A very good starting point to learn about ASR is the HTKbook [1].</p>
                        <p>Different statistical modeling techniques were used for different components of the ASR system. For acoustic models, Hidden Markov Models (HMMs) with Gaussian Mixture Models (GMMs) state representation [2] was used as well as Neural Networks [3]. With the Deep Learning revolution, Neural Networks [4] got a boost in performance by going deeper [5], having large set of acoustic units in its output (using some ways of combining phonemes) [6], and training on very large volume of data [7].
For language modeling there was a similar trend, where the n-gram language models [8] got dethroned by recurrent neural network language models [9].</p>
                        <p>More recently, the research community is going towards a more holistic approach that combine all the four components into one end-to-end ASR system, where inputs are the acoustics signal representation and the output is the word sequence without building four different distinct components [10, 11, 12].</p>
                        <p>For building an ASR system in practice, you can also learn a lot from kaldi [13]. It is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0.</p>
                        <small>
                        <p style="margin-bottom:0;"> [1] Steve Young, et. al "The HTK book" https://www.danielpovey.com/files/htkbook.pdf </p>
                        <p style="margin-bottom:0;"> [2] Mark Gales, Steve Young "The Application of Hidden Markov Models in Speech Recognition" 2007, https://mi.eng.cam.ac.uk/~mjfg/mjfg_NOW.pdf</p>
                        <p style="margin-bottom:0;"> [3] Hervé Bourlard, Nelson Morgan, "Connectionist Speech Recognition: A Hybrid Approach", 1994</p>
                        <p style="margin-bottom:0;"> [4] Geoffrey Hinton, et al. "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The shared views of four research groups", 2012</p>
                        <p style="margin-bottom:0;"> [5] Abdel-rahman Mohamed, George Dahl, Geoffrey Hinton, "Acoustic Modeling using Deep Belief Networks" 2010</p>
                        <p style="margin-bottom:0;"> [6] George Dahl, Dong Yu, Li Deng, Alex Acero, "Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition" 2010</p>
                        <p style="margin-bottom:0;"> [7] Frank Seide, Gang Li, Xie Chen, Dong Yu, "Feature engineering in context-dependent deep neural networks for conversational speech transcription", 2011</p>
                        <p style="margin-bottom:0;"> [8] Andreas Stolcke, "SRILM - an Extensible Language Modeling Toolkit", 2002</p>
                        <p style="margin-bottom:0;"> [9] Tomas Mikolov, et. al, "RNNLM - Recurrent Neural Network Language Modeling Toolkit" 2010</p>
                        <p style="margin-bottom:0;"> [10] Alex Graves, Navdeep Jaitly, "Towards End-To-End Speech Recognition with Recurrent Neural Networks" 2014</p>
                        <p style="margin-bottom:0;"> [11] Dzmitry Bahdanau, et al "End-to-End Attention-based Large Vocabulary Speech Recognition" 2015</p>
                        <p style="margin-bottom:0;"> [12] William Chan, et al "Listen, Attend and Spell" 2015</p>
                        <p style="margin-bottom:0;"> [13] http://kaldi-asr.org/</p>
                    </small>
                        <hr>

                    </div>
  
  </div>
  <div class="tab-pane fade" id="nav-adi" role="tabpanel" aria-labelledby="nav-adi-tab">
  <div class="blog-post">
                        <br/>
                        <h4 class="">Arabic Dialect Identifcation</h4>
                        <br/>
                        <!-- /.blog-post<p>Arabic dialects are sufficiently diverse to the extent that one can argue to describe them as different languages rather than dialects of the same language. Thus, automatically identifying the input dialect can greatly improve ASR.</p>
                        <p>It can be argued that a language is a dialect with an army and navy. If we take this perspective into consideration, we can describe the different Arabic dialects as different languages. However, Arabic today is a language with different dialects and different armies!</p>
                        <p>Dialect identification can be regarded as a special case of language recognition. However, dialect identification is a harder problem since there is no single view of how many spoken dialects there are in Arabic. You can learn about the latest research in Arabic Dialect Identification (ADI) <a href="{{ url_for('publications')}}">here</a>.</p>
                         -->
                        <p>Arabic language has several spoken dialects. There are four major dialects for Arabic, including Egyptian, Gulf, Levantine and North African in addition to modern standard Arabic (MSA) which is the official language in Arabic speaking countries. Sometimes these dialects are significantly different to be considered as different languages rather than dialects of the same language. Thus, automatically identifying the input dialect from the speech signal has been an interesting research problem both on its own and to improve automatic speech recognition (ASR) [1].</p>
                        <p>Approaches to dialect identification are closely related to those of language recognition. These include Gaussian mixture models, the phonotactic approach and phone recognition [2], the i-vector combined with dimensionality reduction [3] and more recently deep learning techniques [4-7]. Arabic dialect identification has been also closely associated with improving dialectal  Arabic ASR interesting work has been done in the context of the GALE project [8] and recent thesis [9]. In spite of this advances Arabic dialect recognition remains a challenging problem and several special sessions and contests have been organized around the subject [10]. These include good pointers to many techniques and data sets. Also there are various repositories [11-13] can be a good start for having an experimental setup.</p>
                        <small>
                            <p style="margin-bottom:0;"> [1] A. Ali, et al. "Automatic dialect detection in Arabic broadcast speech." in Interspeech 2016.</p> 
                            <p style="margin-bottom:0;"> [2] Marc A. Zissman, “A comparison of four approaches to automatic language identification of telephone speech,” in IEEE Transactions on Speech and Audio Processing, vol. 4, no. 1, Jan 1996.</p> 
                            <p style="margin-bottom:0;"> [3] N. Dehak, P.A. Torres-Carrasquillo, D. Reynolds and R. Dehak, “Language recognition via i-vectors and dimensionality reduction,” in Interspeech 2011.</p> 
                            <p style="margin-bottom:0;"> [4] O. Ghahabi, A. Bonafonte, J. Hernando and A. Moreno, “Deep neural networks for i-vector language identification of short utterances in cars,” in Interspeech 2016.</p> 
                            <p style="margin-bottom:0;"> [5] S. Shon, A. Ali, and J. Glass. "MIT-QCRI Arabic dialect identification system for the 2017 multi-genre broadcast challenge." Automatic Speech Recognition and Understanding Workshop (ASRU), 2017.</p> 
                            <p style="margin-bottom:0;"> [6] M. Najafian, et al. "Exploiting convolutional neural networks for phonotactic based dialect identification." in ICASSP 2018.</p> 
                            <p style="margin-bottom:0;"> [7] S. Shon, A. Ali, and J. Glass. "Convolutional Neural Network and Language Embeddings for End-to-End Dialect Recognition." Proc. Odyssey 2018 The Speaker and Language Recognition Workshop. 2018.</p> 
                            <p style="margin-bottom:0;"> [8] F. Biadsy, J. Hirschberg and N. Habash, “Spoken Arabic dialect identification using phonotactic modeling, in Proceedings of EACL workshop on computational approaches to Semitic languages, 2009.</p> 
                            <p style="margin-bottom:0;"> [9] A. Ali. Multi-dialect Arabic broadcast speech recognition. PhD thesis, The University of Edinburgh, 2018.</p> 
                            <p style="margin-bottom:0;"> [10] Zampieri, Marcos, et al. "Language Identification and Morphosyntactic Tagging: The Second VarDial Evaluation Campaign." Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects. Association for Computational Linguistics, 2018.</p> 
                            <p style="margin-bottom:0;"> [11] https://github.com/qcri/dialectID/</p> 
                            <p style="margin-bottom:0;"> [12] https://github.com/swshon/dialectID_e2e</p> 
                            <p style="margin-bottom:0;"> [13] https://github.com/swshon/dialectID_siam </p> 
                        </small>
                         
                    </div><!-- /.blog-post -->
    </div>
  <div class="tab-pane fade" id="nav-tts" role="tabpanel" aria-labelledby="nav-tts-tab">
  <div class="blog-post">
                        <br/>
                        <h4 class="">Text To Speech</h4>
                        <br/>
                        <p>The Text to Speech (TTS) technology aims to convert a sequence of words into speech. Traditional TTS pipelines or engines consist of few steps to generate the speech:
                            <ol>
                            <li>The text normalization or tokenization step aims to convert raw text containing symbols like numbers and abbreviations into the equivalent words.</li> 
                            <li>In text-to-phoneme or grapheme-to-phoneme conversion step, phonetic transcriptions for each word are assigned.</li>
                            <li>Prosodic phrasing step aims to divide and mark the text into prosodic units, like phrases and sentences.</li>
                            <li>Prediction of the target prosody (pitch contour, phoneme durations) step. The target prosody is used to generate/control the output speech.</li>
                            <li>Finally, the synthesizer is used to convert the symbolic linguistic representation into sound.</li>
                            <ol>
                        </p>
                        <p>Synthesized speech can be created by concatenating units of recorded speech that are stored in a database as in [1] [2]. Common units used in concatenative synthesizers are phones or diphones. Alternatively, statistical parametric synthesizers also known as HMM-based synthesizers (based on hidden Markov models) can be used to create the synthesized speech [3][4]. In these systems, the frequency spectrum (vocal tract), fundamental frequency (voice source), and duration (prosody) of speech are modeled simultaneously by HMMs. Speech waveforms are generated from HMMs themselves based on the maximum likelihood criterion.  Recently, neural networks have been used as acoustic models for statistical parametric synthesizers [5]. In addition, end-to-end DNN-based speech synthesizers such as Tacotron [6] by Google and Deep Voice [7] from Baidu are an active area of research. A state-of-the-art synthesizer based on Tacotron, developed for the Arabic language, is available on github [8].</p>
                        <p>Since Modern Standard Arabic (MSA) is written without diacritics, the first step to develop an Arabic TTS engine [2] is to restore the diacritics of each word in the text [9][10][11]. The diacritized text is then passed to a phonetic transcription module to generate the phoneme sequence for each phrase [12].  Hence, a synthesizer (i.e. concatenative, parametric, neural networks) can be used to synthesis the speech.</p>

                        <small>
                            <p style="margin-bottom:0;"> [1] A. Hunt and A. Black, :Unit selection in a concatenative speech synthesis system using a large speech database". In ICASSP-96, volume 1, pages 373--376, Atlanta, Georgia, 1996.</p>
                            <p style="margin-bottom:0;"> [2] Hifny, Yasser, et al. "ArabTalk®: An Implementation for Arabic Text To Speech System."The proceedings of the 4th Conference on Language Engineering. 2004.</p>
                            <p style="margin-bottom:0;"> [3] HMM/DNN-based Speech Synthesis System (HTS), http://hts.sp.nitech.ac.jp/</p>
                            <p style="margin-bottom:0;"> [4] Abdel-Hamid, Ossama, Sherif Mahdy Abdou, and Mohsen Rashwan. "Improving Arabic HMM based speech synthesis quality." Ninth International Conference on Spoken Language Processing. 2006.</p>
                            <p style="margin-bottom:0;"> [5] Merlin: The Neural Network (NN) based Speech Synthesis System, https://github.com/CSTR-Edinburgh/merlin</p>
                            <p style="margin-bottom:0;"> [6] Ping, Wei, et al. "Deep voice 3: Scaling text-to-speech with convolutional sequence learning." (2018).</p>
                            <p style="margin-bottom:0;"> [7] Wang, Yuxuan, et al. "Tacotron: Towards end-to-end speech synthesis." arXiv preprint arXiv:1703.10135</p>
                            <p style="margin-bottom:0;"> [8] https://github.com/youssefsharief/arabic-tacotron-tts</p>
                            <p style="margin-bottom:0;"> [9] Rashwan, Mohsen AA, et al. "A stochastic Arabic diacritizer based on a hybrid of factorized and unfactorized textual features."IEEE Transactions on Audio, Speech, and Language Processing 19.1 (2011): 166-175.</p>
                            <p style="margin-bottom:0;"> [10] Darwish, Kareem, Hamdy Mubarak, and Ahmed Abdelali. "Arabic diacritization: Stats, rules, and hacks."Proceedings of the Third Arabic Natural Language Processing Workshop. 2017.</p>
                            <p style="margin-bottom:0;"> [11] Hifny, Yasser. "Hybrid LSTM/MaxEnt Networks for Arabic Syntactic Diacritics Restoration."IEEE Signal Processing Letters 25.10 (2018): 1515-1519.</p>
                            <p style="margin-bottom:0;"> [12] https://github.com/nawarhalabi/Arabic-Phonetiser</p> 
                        </small>
                    </div><!-- /.blog-post -->
  </div>
  <div class="tab-pane fade" id="nav-lm" role="tabpanel" aria-labelledby="nav-lm-tab">
  <div class="blog-post">
                        <br/>
                        <h4 class="">Language Model </h4>
                        <br/>
                        <p>Language Modeling aims at accurately estimating the probability distribution of a word sequences or sentences produced in a natural language such as Arabic [1]. Having a way to estimate the relative likelihood of different word sequences is useful in many natural language processing applications, especially those where natural text is generated such as the case of speech recognition. The goal of a speech recognizer is to match input speech sounds with word sequences. To accomplish this goal, the speech recognizer will leverage the language model to provide the capability to distinguish between words and phrases that sound similar. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the acoustic model.</p>
                        <p>Language models rely heavily on the context, or history, to estimate the probability distribution. The context can be long or short, knowledge-rich or knowledge-poor. We may base the estimation on a single preceding word (e.g., bigram), or potentially using knowledge of all words from the start of the passage preceding the word in question. Knowledge-rich models can incorporate information about morphology, syntax or semantics to inform the estimation of the probability distribution of word sequence, whereas knowledge poor models will rely solely on the words as the appear in the text. It is reasonable to state that current language modeling techniques can be split into two categories: count based and continuous-space based language models.</p>
                        <p>The count-based approaches represent the traditional techniques and usually involves the estimation of n-gram probabilities, where the goal is to accurately predict the next word in a sequence of words. In a model that estimates probabilities for two-word sequences (bigrams), it is unclear whether a given bigram has a count of zero because it is not a valid sequence in the language, or because it is not in the training data. As the length of the modeled sequences grows more complex, this sparsity issue also grows. Of all possible combinations of 5-grams in a language, very few are likely to appear at all in a given text, and even fewer will repeat often enough to provide reliable frequency statistics. Therefore, as the language model is trying to predict the next word, the challenge is to find appropriate, reliable estimates of word sequence probabilities to enable the prediction. Approaches to this challenge are three-folds: smoothing techniques are used to offset zero-probability sequences and spread probability mass across a model [2-4]; enhanced modeling techniques that incorporate machine learning or complex algorithms are used to create models that can best incorporate additional linguistic information [5-6]; and particularly for Arabic language modeling, morphological information is extracted and provided to the models in place of or in addition to lexical information [7-8].</p>
                        <p>The continuous space-based language modeling approach are based on the use of neuronal networks to estimate the probability distribution of a word sequence [9-10]. This approach, also denoted neuronal language model, are based on feed-forward neural network [9] or recurrent neural network [11-13] that achieved state of the art performance. Recently, a new technique based on transformers (BERT) start to be explored for language modeling as well [16]. Initially, the feed-forward neural network based LM tackled efficiently the problems of data sparsity but not necessary the context. It uses a fixed length context. Every word in the vocabulary is associated with a distributed word feature vector, and the joint probability function of words sequence is a function of the feature vectors of these words in the sequence [9-10].</p>
                        <p>The recurrent neural network based LM was able in a certain degree to address the problem of limited context. It does not use fixed length context as their internal memory is able to remember important things about the input they received. In this type of architecture, neurons with input from recurrent connections assumed to represent short term memory and hence enables them to better leverage the history or context [9, 14, 15]. Also, subsequent research has been focusing on sub-word modelling and corpus-level modelling based on recurrent neural network and its variant, such as the long short-term memory network (LSTM) [15]. However, a very long training time and large amounts of data are still the main limitations. It is also reasonable to say that, sub-word modelling and large-context language model are still interesting challenges to solve, which is very important for a language such as Arabic [17].</p>
                        <p>The reader can also refer to these [18-22] as a start to build your own language models.</p>
                        <small>
                        <p style="margin-bottom:0;"> [1] I. Zitouni (Ed.), Natural language processing of Semitic languages, theory and applications of natural language processing, Chapter 5. Springer, Berlin, Heidelberg (2014) </p>
                        <p style="margin-bottom:0;"> [2] Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181–184.  </p>
                        <p style="margin-bottom:0;"> [3] Ciprian Chelba and Johan Schalkwyk, 2013. Empirical Exploration of Language Modeling for the google.com Query Stream as Applied to Mobile Voice Search, pages 197–229. Springer, New York </p>
                        <p style="margin-bottom:0;"> [4] Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August. </p>
                        <p style="margin-bottom:0;"> [5] P.F. Brown V.J. DellaPietra P.V. DeSouza J.C. Lai R.L. Mercer "Class-based n-gram models of natural language" Computational Linguistics vol. 18 no. 4 pp. 467-479 1992.  </p>
                        <p style="margin-bottom:0;"> [6] R. A. Solsona, E. Fosler-Lussier, H. J. Kuo, A. Potamianos and I. Zitouni, "Adaptive language models for spoken dialogue systems," 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing, Orlando, FL, 2002, pp. I-37-I-40. doi: 10.1109/ICASSP.2002.5743648 </p>
                        <p style="margin-bottom:0;"> [7] G. Choueiter, D. Povey, S. F. Chen and G. Zweig, "Morpheme-Based Language Modeling for Arabic Lvcsr," 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, Toulouse, 2006, pp. I-I. </p>
                        <p style="margin-bottom:0;"> doi: 10.1109/ICASSP.2006.1660205 </p>
                        <p style="margin-bottom:0;"> [8] K. Kirchhoff, D. Vergyri, J. Bilmes, K. Duth, A. Stolcke, “Morphology-based language modeling for conversational Arabic speech recognition” Computer Speech & Language. Vol. 20 no. 4 pp. 589-608 Oct 2006. </p>
                        <p style="margin-bottom:0;"> [9] Mikolov, T. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology, 2012.  </p>
                        <p style="margin-bottom:0;"> [10] W. Mulder, S. Bethard, M.F. Moens. A survey on the application of recurrent neural networks to statistical language modeling. Computer Speech & Language. Vol. 30 no. 1 pp. 61-98 March 2015. </p>
                        <p style="margin-bottom:0;"> [11] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. 2015. Character-Aware Neural Language Models. CoRR, abs/1508.06615. </p>
                        <p style="margin-bottom:0;"> [12] Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155, 2003. </p>
                        <p style="margin-bottom:0;"> [13]Mikolov, T., Karafi´at, M., Burget, L., Cernock`y, J., and Khudanpur, S. Recurrent neural network based language model. In INTERSPEECH, pp. 1045–1048, 2010. </p>
                        <p style="margin-bottom:0;"> [14] Martin Sundermeyer, Hermann Ney, and Ralf Schlüter. 2015. From feedforward to recurrent LSTM neural networks for language modeling. Trans. Audio, Speech and Lang. Proc. 23, 3 (March 2015), 517-529. DOI: https://doi.org/10.1109/TASLP.2015.2400218 </p>
                        <p style="margin-bottom:0;"> [15] S. Yousfi, S.A. Berrani, C. Garcia. Contribution of recurrent connectionist language models in improving LSTM-based Arabic text recognition in videos. Pattern Recognition. Vol. 64 pp. 245-254 April 2017. </p>
                        <p style="margin-bottom:0;"> [16] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv e-prints. </p>
                        <p style="margin-bottom:0;"> [17] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, ´ and Y. Wu. Exploring the limits of language modeling. arXiv preprint, 1602.02410, 2016. arxiv.org/abs/1602.02410. </p>
                        <p style="margin-bottom:0;"> [18] CMU Statistical Language Modeling Toolkit: http://www.speech.cs.cmu.edu/SLM/toolkit.html  </p>
                        <p style="margin-bottom:0;"> [19] HTK Toolkit: http://htk.eng.cam.ac.uk/download.shtml  </p>
                        <p style="margin-bottom:0;"> [20] SRILM - The SRI Language Modeling Toolkit:  http://www.speech.sri.com/projects/srilm/ </p>
                        <p style="margin-bottom:0;"> [21] Stanford CoreNLP – Natural language software: https://stanfordnlp.github.io/CoreNLP/ </p>
                        <p style="margin-bottom:0;"> [22] The Berkeley NLP Group: http://nlp.cs.berkeley.edu/software.shtml </p>
                    </small>
                    </div><!-- /.blog-post -->
                    
  </div>
</div>
</div>
{% endblock content %}
